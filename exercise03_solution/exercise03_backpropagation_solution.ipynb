{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJFZEFq4ueIi"
   },
   "source": [
    "# Exercise 3\n",
    "\n",
    "The exercise focuses on backpropagation. We begin with pen and paper tasks followed by code tasks which build on the previous exercise.\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pen and Paper Tasks\n",
    "\n",
    "Perform a forward and backward pass to calculate the gradients for the weights $w_0, w_1, w_2, w_s$ in the following MLP. Each node represents one unit with a weight $w_i, i \\in \\{0, 1, 2\\}$ connecting it to the previous node. The connection from unit 0 to unit 2 is called a __skip connection__, which means unit 2 receives input from two sources and thus has an additional weight $w_s$. The weighted inputs are added before the nonlinearity is applied.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We assume that we want to solve a regression task. We use an L1-loss $L(\\hat{y}, y) = |y - \\hat{y}|$\n",
    "\n",
    "The nonlinearities for the first two units are rectified linear functions/units (ReLU): $g_0(z) = g_1(z) = \\begin{cases} 0, z<0\\\\ z, else \\end{cases}$.\n",
    "\n",
    "We do not use a nonlinearity for the second unit: $g_2(z_2) = z_2$ and there are **no biases**.\n",
    "\n",
    "**Note:** We use the notation of the Deep Learning book here, i.e. $z = Wx+b$. If you attended the Machine Learning course, you might be used to the different notation used in the Bishop Book, where $z$ denotes the value after applying the activation function. Here, $z$ is the value before applying the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the backpropagation algorithm for the above network** (2 points)\n",
    "\n",
    "First perform the forward pass, followed by the backward pass to obtain gradients for the weights $w_0, w_1, w_2, w_s$.\n",
    "\n",
    "You can paste a scanned image of your solution to this exercise on paper into this Jupyter notebook. Or if you want to make it easier for the tutors to correct, you can also answer in Latex code in this cell. Some example Latex code is entered here to help you get started.\n",
    "\n",
    "Please assign equations/values to variables and reuse them later when possible to make your solution more readable.\n",
    "\n",
    "#### START TODO ####\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial L}{\\partial z_2} &= \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\dots{} \\\\\n",
    "    \\dots{} &= \\dots{} \\\\\n",
    "    \\frac{\\partial L}{\\partial w_0} &= \\dots{} \\\\\n",
    "\\end{align*}   \n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{forward pass} &&\n",
    "    \\textbf{backward pass} \\\\\n",
    "    L(y, \\hat{y}) = |y-\\hat{y}| &&\n",
    "     \\\\\n",
    "    \\hat{y}=g_2(z_2)=z_2 &&\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} = \\begin{cases} 1, \\hat{y} > y \\\\ -1, else \\end{cases} \\\\\n",
    "    z_2 = w_2 \\cdot h_1 + w_s \\cdot h_0 &&\n",
    "    \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot 1 \\\\\n",
    "    h_1 = g_1(z_1) = \\begin{cases} 0, z_1<0 \\\\ z_1, else \\end{cases} && \n",
    "    \\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_1} = \\frac{\\partial L}{\\partial z_2} \\cdot w_2 \\\\\n",
    "    z_1 = w_1 \\cdot h_0 &&\n",
    "    \\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} = \\frac{\\partial L}{\\partial h_1} \\cdot \\begin{cases} 0, z_1 < 0 \\\\ 1, else \\end{cases} \\\\\n",
    "    h_0 = g_0(z_0) = \\begin{cases} 0, z_0<0 \\\\ z_0, else \\end{cases} && \n",
    "    \\frac{\\partial L}{\\partial h_0} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_0} + \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial h_0} = \\frac{\\partial L}{\\partial z_2} \\cdot w_s + \\frac{\\partial L}{\\partial z_1} \\cdot w_1 \\\\\n",
    "    z_0 = w_0 \\cdot x &&\n",
    "    \\frac{\\partial L}{\\partial z_0} = \\frac{\\partial L}{\\partial h_0} \\cdot \\frac{\\partial h_0}{\\partial z_0} = \\frac{\\partial L}{\\partial h_0} \\cdot \\begin{cases} 0, z_0 < 0 \\\\ 1, else \\end{cases} \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot h_1 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_s} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_s} = \\frac{\\partial L}{\\partial z_2} \\cdot h_0 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot h_0 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial z_0} \\cdot \\frac{\\partial z_0}{\\partial w_0} = \\frac{\\partial L}{\\partial z_0} \\cdot x \\\\\n",
    "\\end{align*}   \n",
    "#### END TODO ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What difference does the skip connection make when propagating back the error?** (1 point)\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:** Regarding the backpropagation, the third layer receives input from two sources, therefore the gradient flows as well from the third to the first layer in the backward pass. The result is an additional term in the corresponding equations.\n",
    "\n",
    "Regarding the effectiveness of training, this helps to reduce the (vanishing gradient problem)[https://en.wikipedia.org/wiki/Vanishing_gradient_problem] . If we have many layers in our network, the magnitude of gradients can shrink towards the first layers to a point where they basically are not trained. Skip connections help bypass this by propagating the gradient uninterrupted from the last layers. With increasingly deep networks, they are applied quite often in modern architectures.\n",
    "#### END TODO ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the gradients for the given datapoint and the given initial weights (calculating the gradients requires to calculate a forward pass first). Also calculate the weights and the loss after one gradient descent step.** (3 points)\n",
    "\n",
    "$$(x_1, y_1) = (1, -3) \\\\\n",
    "w_0 = w_1 = w_2 = w_s = 0.5 \\\\\n",
    "Learning Rate = 1 \\\\  $$\n",
    "\n",
    "#### START TODO ####\n",
    "\\begin{align*}\n",
    "    \\textbf{forward pass} &&\n",
    "    \\textbf{backward pass} \\\\\n",
    "    L(y, \\hat{y}) = |y-\\hat{y}| = 3.375 &&\n",
    "     \\\\\n",
    "    \\hat{y}=g_2(z_2)=z_2 = 0.375 &&\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} = \\begin{cases} 1, \\hat{y} > y \\\\ -1, else \\end{cases} = 1 \\\\\n",
    "    z_2 = w_2 \\cdot h_1 + w_s \\cdot h_0 = 0.375 &&\n",
    "    \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot 1 = 1 \\\\\n",
    "    h_1 = \\begin{cases} 0, z_1<0 \\\\ z_1, else \\end{cases} = 0.25 && \n",
    "    \\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial z_2} \\cdot w_2 = 0.5 \\\\\n",
    "    z_1 = w_1 \\cdot h_0 = 0.25 &&\n",
    "    \\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial h_1} \\cdot \\begin{cases} 0, z_1 < 0 \\\\ 1, else \\end{cases} = 0.5 \\\\\n",
    "    h_0 = \\begin{cases} 0, z_0<0 \\\\ z_0, else \\end{cases} = 0.5 && \n",
    "    \\frac{\\partial L}{\\partial h_0} = \\frac{\\partial L}{\\partial z_2} \\cdot w_s + \\frac{\\partial L}{\\partial z_1} \\cdot w_1 = 0.75 \\\\\n",
    "    z_0 = w_0 \\cdot x = 0.5 &&\n",
    "    \\frac{\\partial L}{\\partial z_0} = \\frac{\\partial L}{\\partial h_0} \\cdot \\begin{cases} 0, z_0 < 0 \\\\ 1, else \\end{cases} = 0.75 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot h_1 = 0.25 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_s} = \\frac{\\partial L}{\\partial z_2} \\cdot h_0 = 0.5 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot h_0 = 0.25 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial z_0} \\cdot x = 0.75 \\\\\n",
    "\\end{align*}   \n",
    "\n",
    "$\n",
    "\\textbf{gradient descent step} \\\\\n",
    "w_2 = w_2 - 1 \\cdot 0.25 = 0.25 \\\\\n",
    "w_s = w_s - 1 \\cdot 0.5 = 0 \\\\\n",
    "w_1 = w_1 - 1 \\cdot 0.25 = 0.25 \\\\\n",
    "w_0 = w_0 - 1 \\cdot 0.75 = -0.25 \\\\\n",
    "$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{forward pass} \\\\\n",
    "    L(y, \\hat{y}) = |y-\\hat{y}| = 3 \\\\\n",
    "    \\hat{y}=g_2(z_2)=z_2 = 0 \\\\\n",
    "    z_2 = w_2 \\cdot h_1 + w_s \\cdot h_0 = 0 \\\\\n",
    "    h_1 = \\begin{cases} 0, z_1<0 \\\\ z_1, else \\end{cases} = 0 \\\\\n",
    "    z_1 = w_1 \\cdot h_0 = 0 \\\\\n",
    "    h_0 = \\begin{cases} 0, z_0<0 \\\\ z_0, else \\end{cases} = 0 \\\\\n",
    "    z_0 = w_0 \\cdot x = -0.25 \\\\\n",
    "\\end{align*}   \n",
    "#### END TODO ####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Tasks\n",
    "\n",
    "The coding tasks build on the previous exercise. We complete implementing a small feedforward neural network and then use backprop to obtain gradients and update the weights of the network for the XOR dataset as last time. We provide code for structure and utility and you have to **fill in the TODO-gaps**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YIChkVlueIl"
   },
   "outputs": [],
   "source": [
    "# Some imports used in the code below\n",
    "from typing import Iterable, List, Optional, Tuple  # type annotations\n",
    "\n",
    "import numpy as np  # linear algebra\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "import scipy.optimize  # gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2w5-mnoueIu"
   },
   "source": [
    "In the previous exercise, you used the **Parameter** and **Module** classes and implemented their forward passes. This time you will implement their backward passes.\n",
    "\n",
    "*Note:* An additional utility module is used to check the correctness of your implementation by approximating *backward* with [finite difference approximations](https://en.wikipedia.org/wiki/Finite_difference#Relation_with_derivatives)  of *forward*. All modules operate on batches of samples. E.g. the input shape of `Linear.forward` is `(batch_size, feature_shape, 1)` (we will use the last dimension in future exercises)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8f1UWYwFueIw"
   },
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"A trainable parameter.\n",
    "\n",
    "    This class not only stores the value of the parameter (self.data) but also tensors/\n",
    "    properties associated with it, such as the gradient (self.grad) of the current backward\n",
    "    pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, grad: Optional[np.ndarray] = None, name=None):\n",
    "        self.data = data  # type: np.ndarray\n",
    "        self.grad = grad  # type: Optional[np.ndarray]\n",
    "        self.name = name  # type: Optional[str]\n",
    "        self.state_dict = dict()  # dict to store additional, optional information\n",
    "        \n",
    "        \n",
    "class Module:\n",
    "    \"\"\"The base class all network modules must inherit from.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Cache of the input of the forward pass.\n",
    "        # We need it during the backward pass in most layers,\n",
    "        #  e.g., to compute the gradient w.r.t to the weights.\n",
    "        self.input_cache = None\n",
    "\n",
    "    def __call__(self, *args) -> np.ndarray:\n",
    "        \"\"\"Alias for forward, convenience function.\"\"\"\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, *args) -> np.ndarray:\n",
    "        \"\"\"Compute the forward pass through the module.\n",
    "\n",
    "        Args:\n",
    "           args: The inputs, e.g., the output of the previous layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the backward pass through the module.\n",
    "\n",
    "        This method computes the gradients with respect to the trainable\n",
    "        parameters and with respect to the first input.\n",
    "        If the module has trainable parameters, this method needs to update\n",
    "        the respective parameter.grad property.\n",
    "\n",
    "        Args:\n",
    "            grad: The gradient of the following layer.\n",
    "\n",
    "        Returns:\n",
    "            The gradient with respect to the first input argument. In general\n",
    "            it might be useful to return the gradients w.r.t. to all inputs, we\n",
    "            omit this here to keep things simple.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        \"\"\"Return the module parameters.\"\"\"\n",
    "        return []  # default to empty list\n",
    "\n",
    "    def check_gradients(self, input_args: Tuple[np.ndarray]):\n",
    "        \"\"\"Verify the implementation of the gradients.\n",
    "\n",
    "        This includes the gradient with respect to the input as well as the\n",
    "        gradients w.r.t. the parameters if the module contains any.\n",
    "\n",
    "        As the scipy grad check only works on scalar functions, we compute\n",
    "        the sum over the output to obtain a scalar.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_args, tuple), (\n",
    "            \"input_args must be a tuple but is {}\".format(type(input_args)))\n",
    "        TOLERANCE = 1e-6\n",
    "        self.check_gradients_wrt_input(input_args, TOLERANCE)\n",
    "        self.check_gradients_wrt_params(input_args, TOLERANCE)\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        \"\"\"(Re-) intialize the param's grads to 0. Helper for grad checking.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def check_gradients_wrt_input(self, input_args: Tuple[np.ndarray],\n",
    "                                  tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. input.\"\"\"\n",
    "\n",
    "        def output_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.forward for scipy.optimize.check_grad.\"\"\"\n",
    "            # we only compute the gradient w.r.t. to the first input arg.\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            return np.sum(self.forward(*args))\n",
    "\n",
    "        def grad_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.backward for scipy.optimize.check_grad.\"\"\"\n",
    "            self._zero_grad()\n",
    "            # run self.forward to store the new input\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            out = self.forward(*args)\n",
    "            # compute the gradient w.r.t. to the input\n",
    "            return np.ravel(self.backward(np.ones_like(out)))\n",
    "\n",
    "        error = scipy.optimize.check_grad(\n",
    "            output_given_input, grad_given_input, np.ravel(input_args[0]))\n",
    "        num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "        if np.squeeze(error) / num_outputs > tolerance:\n",
    "            raise RuntimeError(\"Check of gradient w.r.t. to input for {} failed.\"\n",
    "                               \"Error {:.4E} > {:.4E}.\"\n",
    "                               .format(self, np.squeeze(error), tolerance))\n",
    "\n",
    "    def check_gradients_wrt_params(self, input_args: Tuple[np.ndarray],\n",
    "                                   tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. params.\"\"\"\n",
    "        for param in self.parameters():\n",
    "            def output_given_params(new_param: np.ndarray):\n",
    "                \"\"\"Wrap self.forward, change the parameters to new_param.\"\"\"\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                return np.sum(self.forward(*input_args))\n",
    "\n",
    "            def grad_given_params(new_param: np.ndarray):\n",
    "                self._zero_grad()\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                out = self.forward(*input_args)\n",
    "                # compute the gradient w.r.t. to param\n",
    "                self.backward(np.ones_like(out))\n",
    "                return np.ravel(param.grad)\n",
    "            # flatten the param as scipy can only handle 1D params\n",
    "            param_init = np.ravel(np.copy(param.data))\n",
    "            error = scipy.optimize.check_grad(output_given_params,\n",
    "                                              grad_given_params,\n",
    "                                              param_init)\n",
    "            num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "            if np.squeeze(error) / num_outputs > tolerance:\n",
    "                raise RuntimeError(\"Check of gradient w.r.t. to param '{}' for\"\n",
    "                                   \"{} failed. Error {:.4E} > {:.4E}.\"\n",
    "                                   .format(param.name, self, error, tolerance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pvmc_mniueI2"
   },
   "source": [
    "# Nonlinearities\n",
    "\n",
    "Implement the backward passes for the *Module*s below. The *backward()* function of a *Module* receives a *grad* argument from the *Module* after it in the network and using the chain rule calculates the gradient to be passed to *Module*s before it. This way the gradient information flows backward through backpropagation.\n",
    "\n",
    "## Sigmoid (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLb-27dFueI4"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def _sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        assert len(z.shape) == 3, (\"z.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(z.shape))\n",
    "        h = self._sigmoid(z)\n",
    "        # here it's useful to store the activation \n",
    "        #  instead of the input\n",
    "        self.input_cache = h\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        h = self.input_cache\n",
    "        # START TODO ################\n",
    "        # raise NotImplementedError\n",
    "        return grad * h * (1 - h)\n",
    "        # END TODO###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMQ5DmToueJC"
   },
   "source": [
    "## Relu (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBYjj761ueJD"
   },
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = z\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        z = self.input_cache\n",
    "        # START TODO ################\n",
    "        # raise NotImplementedError\n",
    "        return grad.reshape(z.shape) * np.where(z > 0, 1, 0)\n",
    "        # END TODO###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zsNa01IueJN"
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWZf9BhTueJP"
   },
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    def _softmax(self, z):\n",
    "        # don't reduce (sum) over batch axis\n",
    "        reduction_axes = tuple(range(1, len(z.shape))) \n",
    "        \n",
    "        # Shift input for numerical stability.\n",
    "        shift_z = z - np.max(z, axis=reduction_axes, keepdims=True)\n",
    "        exps = np.exp(shift_z)\n",
    "        h = exps / np.sum(exps, axis=reduction_axes, keepdims=True)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        h = self._softmax(z)\n",
    "        return h\n",
    "\n",
    "    def backward(self, grad) -> np.ndarray:\n",
    "        error_msg = (\"Softmax doesn't need to implement a gradient here, as it's\"\n",
    "                     \"only needed in CrossEntropyLoss, where we can simplify\"\n",
    "                     \"the gradient for the combined expression.\")\n",
    "        raise NotImplementedError(error_msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuHOIiLhueJX"
   },
   "source": [
    "# Linear Layer (2 points)\n",
    "\n",
    "Implement the backward pass for the Linear layer Module. Remember that the Linear layer holds objects of the Parameter class and you would need to update their gradients during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inuuMNmUueJY"
   },
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        w_data = 0.5 * np.random.randn(out_features, in_features)\n",
    "        self.W = Parameter(w_data, None, \"W\")\n",
    "        \n",
    "        b_data = 0.01 * np.ones((out_features, 1))\n",
    "        self.b = Parameter(b_data, None, \"b\")\n",
    "        \n",
    "        self._zero_grad()\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 3, (\"x.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(x.shape))\n",
    "        self.input_cache = x\n",
    "        # Remember: Access weight data through self.W.data\n",
    "        z = self.W.data @ x + self.b.data\n",
    "        return z\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # Return all parameters of Linear\n",
    "        return self.W, self.b\n",
    "        \n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        x = self.input_cache\n",
    "        # remember that input has a batch dimension when transposing, i.e.,\n",
    "        # we need to use np.transpose instead of x.T\n",
    "        x_transpose = np.transpose(x, [0, 2, 1])\n",
    "        \n",
    "        # START TODO ################ \n",
    "        # self.W.grad += ...\n",
    "        # ...\n",
    "        # raise NotImplementedError\n",
    "        self.W.grad += np.sum(grad @ x_transpose, axis=0)\n",
    "        self.b.grad += np.sum(grad, axis=0)\n",
    "        return self.W.data.T @ grad        \n",
    "        # END TODO ##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8FtzDUfueJd"
   },
   "source": [
    "# Cost Function\n",
    "\n",
    "## Cross Entropy (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xezzvQYTueJe"
   },
   "outputs": [],
   "source": [
    "# Define the Cross-Entropy cost functions\n",
    "class CrossEntropyLoss(Module):\n",
    "    \"\"\"Compute the cross entropy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the cross entropy, mean over batch size.\"\"\"\n",
    "        a = self.softmax(a)\n",
    "        self.input_cache = a, y\n",
    "        # compute the mean over the batch\n",
    "        return -np.sum(np.log(a[y == 1])) / len(a)\n",
    "\n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        # we introduce the argument _ here, to have a unified interface with\n",
    "        # other Module objects. This simplifies code for gradient checking. \n",
    "        # We don't need this arg.\n",
    "        a, y = self.input_cache\n",
    "        \n",
    "        # START TODO ################ \n",
    "        # raise NotImplementedError\n",
    "        grad = (a - y) / len(a)\n",
    "        # END TODO ##################\n",
    "\n",
    "        # Recreate the batch dimension\n",
    "        grad = np.expand_dims(grad, -1)\n",
    "        \n",
    "        assert len(grad.shape) == 3, (\"CrossEntropyLoss.backward should return (batch_size, grad_size, 1)\"\n",
    "                                      \" but is {}.\".format(grad.shape))\n",
    "        return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzJ8oxApueJi"
   },
   "source": [
    "# Sequential Network (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14vfdOnGueJk"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"A sequential container to stack modules.\n",
    "\n",
    "    Modules will be added to it in the order they are passed to the\n",
    "    constructor.\n",
    "\n",
    "    Example network with one hidden layer:\n",
    "    model = Sequential(\n",
    "                  Linear(5,10),\n",
    "                  ReLU(),\n",
    "                  Linear(10,10),\n",
    "                )\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: List[Module]):\n",
    "        super().__init__()\n",
    "        self.modules = args\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Remember: module(x) is equivalent to module.forward(x)\n",
    "        for module in self.modules:\n",
    "            x = module(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "        # Perform the backward pass in reverse of the order that the Modules were present in the args to\n",
    "        # the Network during its initialization. Python provieds a utility reversed() in order to reverse a list.\n",
    "        # raise NotImplementedError\n",
    "        for module in reversed(self.modules):\n",
    "            grad = module.backward(grad)\n",
    "        # END TODO ##################\n",
    "        return grad\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # iterate over modules and retrieve their parameters, iterate over\n",
    "        # parameters to flatten the list\n",
    "        return [param for module in self.modules\n",
    "                for param in module.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding\n",
    "Although binary classification for XOR can be done without using one hot encdoing, here we will be using the Softmax which allows us to be more general and perform multi-class classification of which binary classification is a special case. To handle categorical data of multiple classes, we will use the one_hot_encoding utility function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"Convert integer labels to one hot encoding.\n",
    "\n",
    "    Example: y=[1, 2] --> [[0, 1, 0], [0, 0, 1]]\n",
    "    \"\"\"\n",
    "    encoded = np.zeros(y.shape + (num_classes,))\n",
    "    encoded[np.arange(len(y)), y] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "y = np.array([1, 2, 0])\n",
    "np.testing.assert_equal(one_hot_encoding(y, 3), [[0, 1, 0], [0, 0, 1], [1, 0, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check (1 point)\n",
    "\n",
    "Gradient checking is a useful utility to check, whether gradients obtained through finite differences and backward pass are matching. We have implemented the gradient checking in the Module class for you. As all classes we defined up to here inherit from Module, we can run `check_gradients` to check if you have implemented their backward passes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = np.random.uniform(-1., 1., size=(2, 10, 1))\n",
    "input_args = (input_vector,)\n",
    "\n",
    "# layers + activations\n",
    "Relu().check_gradients(input_args)\n",
    "Sigmoid().check_gradients(input_args)\n",
    "Linear(10, 30).check_gradients(input_args)\n",
    "\n",
    "# START TODO ################ \n",
    "# Instantiate a Sequential network with layers: linear, sigmoid, linear and \n",
    "# perform the gradient check on it.\n",
    "# raise NotImplementedError\n",
    "model = Sequential(Linear(10, 30),\n",
    "                   Sigmoid(),\n",
    "                   Linear(30, 10)\n",
    "                   )\n",
    "model.check_gradients(input_args)\n",
    "# END TODO ##################\n",
    "\n",
    "# losses\n",
    "input_args_losses = (one_hot_encoding(np.array([1, 2]), 3),  # a\n",
    "                     one_hot_encoding(np.array([1, 1]), 3))  # y (ground truth)\n",
    "CrossEntropyLoss().check_gradients(input_args_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmNlenRuueKF"
   },
   "source": [
    "# Experiments (2 points)\n",
    "\n",
    "We use the XOR dataset from last time and perform backpropagation on the 1 hidden layer model from last time, except that we now use 2 output units to perform binary classification as multi-class classification. First, perform 1 step of backprop and use the gradient you obtain to update the weights of the network. See how the parameters and their gradients change in the network. Then perform mutiple steps and see how the loss on the dataset evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-YiK7YfHgXl"
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "# true labels\n",
    "Y = np.array([0, 1, 1, 0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E71BjGgsKJOI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the model here!\n",
    "linear_units = 2\n",
    "model = Sequential(Linear(2, linear_units),\n",
    "                   Relu(),\n",
    "                   Linear(linear_units, 2),\n",
    "                   )\n",
    "\n",
    "# Implement a function to reset the gradients of parameters\n",
    "def zero_grad(params) -> None:\n",
    "    \"\"\"Clear the gradients of all optimized parameters.\"\"\"\n",
    "# START TODO ################\n",
    "    for param in params:\n",
    "        assert isinstance(param, Parameter)\n",
    "        param.grad = np.zeros_like(param.data)\n",
    "# END TODO ##################\n",
    "\n",
    "lr = 1\n",
    "\n",
    "x = np.expand_dims(X, -1)\n",
    "y = one_hot_encoding(Y, 2)\n",
    "\n",
    "y_predicted = model(x)\n",
    "print(\"Initial predictions\", np.argmax(y_predicted, axis=1))\n",
    "h_1 = np.squeeze(y_predicted)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn(h_1, y)\n",
    "print(\"Loss before any steps taken:\", loss)\n",
    "print(\"Params before:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.data)\n",
    "print(\"Grad before:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.grad)\n",
    "# START TODO ################\n",
    "# Perform a backward pass on it to obtain the gradients and then use them to update the parameters of the network\n",
    "grad = loss_fn.backward()\n",
    "model.backward(grad)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.data -= lr * p.grad\n",
    "# END TODO ##################\n",
    "\n",
    "print(\"Grad after:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.grad)\n",
    "print(\"Params after:\")\n",
    "for p in model.parameters():\n",
    "    print(p.name)\n",
    "    print(p.data)\n",
    "y_predicted = model(x)\n",
    "h_1 = np.squeeze(y_predicted)\n",
    "loss = loss_fn(h_1, y)\n",
    "print(\"Loss after 1 step:\", loss)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # START TODO ################\n",
    "    # Now perform multiple steps of updating the parameters and observe the loss\n",
    "    # Code will be very similar to single step above but with one very significant change\n",
    "    zero_grad(model.parameters())\n",
    "\n",
    "    grad = loss_fn.backward()\n",
    "    model.backward(grad)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    # END TODO ##################\n",
    "\n",
    "    y_predicted = model(x)\n",
    "    h_1 = np.squeeze(y_predicted)\n",
    "    loss = loss_fn(h_1, y)\n",
    "    print(\"Loss after\", i + 2, \"steps:\", loss)\n",
    "\n",
    "\n",
    "# Print final predictions after multiple steps of updates\n",
    "y_predicted = model(x)\n",
    "print(\"Final predictions\", np.argmax(y_predicted, axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions on experiments (2 points)\n",
    "\n",
    "**What do you observe after 1 step of updating parameters?**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:** The loss went (possibly up) down as we update the parameters in the direction given by the negative gradient.\n",
    "#### END TODO ####\n",
    "\n",
    "**What do you observe after multiple steps of updating parameters? Does the loss always decrease? Explain why it may not.**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:** The loss usually goes down as we update the parameters in the direction given by the negative gradient. But it may not always. When we are modelling non-linear functions, it may happen that gradients, which provide 1st order information about the behaviour of the function, are not enough to let us achieve a decrease in the loss function value. \n",
    "#### END TODO ####\n",
    "If the loss does decrease, then a 1st order Taylor's expansion of the function gives a good approximation to the non-linear function around the current parameter values.\n",
    "\n",
    "**Run the experiment multiple times. Do you always end up with the correct final predictions? Explain why or why not?**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:** No, many runs tend to not give the correct predictions at the end. This could be because they got stuck in local minima or the number of update steps wasn't enough.\n",
    "#### END TODO ####\n",
    "Such phenomena can be caused due to poorly chosen hyperparameters which include network weights' initialization, learning rates, update steps among others. You can read more about it here: https://datascience.stackexchange.com/questions/11589/creating-neural-net-for-xor-function\n",
    "\n",
    "**What is the role of the variable lr above? When would you set it to a relatively larger value and when would you set it to a relatively smaller value? Explain.**\n",
    "\n",
    "#### START TODO ####\n",
    "**Answer:** It's the learning rate and it controls the size of the updates performed on the network parameters. In theory, having a larger learning rate make sense where the 1st order Taylor expansion is a good approximation to the true non-linear function and having a smaller one where it's not. We commonly also decrease it across training or schedule it in various ways. Other ways to effectively increase or decrease the learning rate may be momentum or adaptive optimizers. Many of these methods perform empirically very well in practice.\n",
    "#### END TODO ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzidWhY_Ekqb"
   },
   "source": [
    "(BONUS): After transforming the input data into the hidden representation space, generate two plots showing the dataset in the input and representation spaces, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsWaa2fnntSy"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(x1: np.ndarray, x2: np.ndarray, h1: np.ndarray, h2: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    x1: np.ndarray with shape (nr_examples,). First input features.\n",
    "    x2: np.ndarray with shape (nr_examples,). Second input feature.\n",
    "    h1: np.ndarray with shape (nr_examples,). First learned features.\n",
    "    h2: np.ndarray with shape (nr_examples,). Second learned feature.\n",
    "    y: np.ndarray with shape (nr_examples,). True labels.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    for k, (i, j) in enumerate(zip(x1, x2)):\n",
    "        ax[0].scatter(i, j, c='b', marker=r\"${}$\".format(y[k]), s=100)\n",
    "    for k, (i, j) in enumerate(zip(h1, h2)):\n",
    "        ax[1].scatter(i, j, c='b', marker=r\"${}$\".format(y[k]), s=100)\n",
    "\n",
    "    ax[0].set_xlabel('x1')\n",
    "    ax[0].set_ylabel('x2')\n",
    "    ax[0].set_title(\"Original x space\")\n",
    "\n",
    "    ax[1].set_xlabel('h1')\n",
    "    ax[1].set_ylabel('h2')\n",
    "    ax[1].set_title(\"Learned h space\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 608,
     "status": "ok",
     "timestamp": 1572439387939,
     "user": {
      "displayName": "Arber Zela",
      "photoUrl": "https://lh3.googleusercontent.com/-S819W68olkc/AAAAAAAAAAI/AAAAAAAAAhw/gBWxwn5CwRE/s64/photo.jpg",
      "userId": "10640764346134349069"
     },
     "user_tz": -60
    },
    "id": "r5kvCHpNDklm",
    "outputId": "de635a47-d288-4152-ed54-156ad639990b"
   },
   "outputs": [],
   "source": [
    "def extract_hidden(full_model: Module, x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Function to extraxt the hidden representation from a MLP.\n",
    "    Args:\n",
    "    full_model: Module. The sequential model used as a classifier\n",
    "    x: np.ndarray with shape (nr_examples, nr_features). Input examples\n",
    "    Returns:\n",
    "    h: np.ndarray with shape (nr_examples, nr_features). Hidden representation of inputs.\n",
    "    \"\"\"\n",
    "    # Extract the hidden features from the sequential model defined above and\n",
    "    # compute the hidden representation after propagating the input through\n",
    "    # the first Linear layer and the ReLU function.\n",
    "    h = Sequential(*full_model.modules[:-1])\n",
    "    h = h(np.expand_dims(x, -1)).reshape(4, 2)\n",
    "    return h\n",
    "\n",
    "h = extract_hidden(model, X)\n",
    "plot(X[:, 0], X[:, 1], h[:, 0], h[:, 1], Y.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-atiph1GueKF"
   },
   "source": [
    "** Your feedback on exercise 3: ** \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "exercise02_python_mlp.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
