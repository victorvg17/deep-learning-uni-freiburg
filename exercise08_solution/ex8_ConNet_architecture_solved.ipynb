{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8\n",
    "This exercise focuses on CNN architectures. We'll continue using Pytorch.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Implement basic architectures.\n",
    "- Experiment with architectures, hyperparameters and batch normalization.\n",
    "- Design and implement a Convolutional network that improves classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and normalizing the CIFAR10 dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 colour images in 10 classes, 50,000 training images and 10,000 test images of size 32 x 32.\n",
    "\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "\n",
    "The pytorch provides a package called torchvision, that automatically download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dog horse   car horse\n"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "img = torchvision.utils.make_grid(images[:4,:,:,:])\n",
    "\n",
    "# show images\n",
    "img = img / 2 + 0.5     # unnormalize\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Convolutional Neural Network (4 points)\n",
    "\n",
    "A simple Convolutional Network is a sequence of layers. The most used layers to build ConvNets are Convolutional Layer, Pooling Layer, and Fully-Connected Layer.\n",
    "\n",
    "Your job is to stack the three main types of layers to form a full ConvNet architecture following the pattern in CIFAR-10:\n",
    "\n",
    " [INPUT - CONV - RELU - POOL - FC]\n",
    " \n",
    "* INPUT [32x32x3]: the RGB raw pixel values of the image.\n",
    "* CONV Convolutional layer with 3 filters and filter size 5 (use padding value 2 to preserve the input dimension).\n",
    "* RELU applies an elementwise activation function.\n",
    "* POOL downsampling operation through the spatial dimensions kernel size = 2.\n",
    "* FC fully-connected layer that computes the class scores, with output size [1x1x10], where each of the 10 numbers corresponds to a class score. Each neuron in this layer will be connected to all the numbers in the previous volume.\n",
    "\n",
    "Implement the `forward` function, remember that the `backward` pass is computed automatically in Pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet1(\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=768, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Nfilters = 3\n",
    "Ksize = 5\n",
    "padding = 2\n",
    "class ConvNet1(nn.Module):\n",
    "    \"\"\"\n",
    "        The CNN convolutional network with architecture defined above\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # START TODO #############\n",
    "        # initialize required parameters / layers needed to build the network\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters, kernel_size=Ksize,padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features= 16*16*Nfilters, out_features=10)\n",
    "        # END TODO #############\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            x: The input tensor with shape [batch_size, feature_dim] (minibatch of data)\n",
    "        Returns:\n",
    "            scores: Pytorch tensor of shape (N, C) giving classification scores for x\n",
    "        \"\"\"\n",
    "        # START TODO #############\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Remember to flatten the feature map using:\n",
    "        # x = x.view(batch_size, dim)\n",
    "        x = x.view(x.shape[0],16*16*Nfilters)\n",
    "        x = self.fc1(x)\n",
    "        # END TODO #############\n",
    "        return x\n",
    "\n",
    "net = ConvNet1()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('palavattom', 'kahtu ninnu njan')\n",
      "key=first, val=come on baby\n",
      "key=second, val=come to me\n",
      "key=third, val=varilla ne?\n"
     ]
    }
   ],
   "source": [
    "def my_func(*args, **kwargs):\n",
    "    print(args)\n",
    "    for key,val in kwargs.items():\n",
    "        print(f'key={key}, val={val}')\n",
    "my_func(\"palavattom\", \"kahtu ninnu njan\",first = \"come on baby\", second=\"come to me\", third=\"varilla ne?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999755859375"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fu = {'test_accuracy': 0.9619, 'train_accuracy': 0.999755859375, 'valid_accuracy': 0.970703125}\n",
    "fu['apple'] = 1\n",
    "fu['orane'] = 5\n",
    "fu['grape'] = 7\n",
    "fu['train_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train and see the result of the network. If your implementation is correct, you should get around 47% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.992\n",
      "[1,  2000] loss: 0.880\n",
      "[1,  3000] loss: 0.862\n",
      "[1,  4000] loss: 0.844\n",
      "[1,  5000] loss: 0.840\n",
      "[1,  6000] loss: 0.835\n",
      "[1,  7000] loss: 0.810\n",
      "[1,  8000] loss: 0.795\n",
      "[1,  9000] loss: 0.817\n",
      "[1, 10000] loss: 0.797\n",
      "[1, 11000] loss: 0.784\n",
      "[1, 12000] loss: 0.787\n",
      "[2,  1000] loss: 0.772\n",
      "[2,  2000] loss: 0.778\n",
      "[2,  3000] loss: 0.753\n",
      "[2,  4000] loss: 0.773\n",
      "[2,  5000] loss: 0.783\n",
      "[2,  6000] loss: 0.788\n",
      "[2,  7000] loss: 0.769\n",
      "[2,  8000] loss: 0.753\n",
      "[2,  9000] loss: 0.765\n",
      "[2, 10000] loss: 0.766\n",
      "[2, 11000] loss: 0.759\n",
      "[2, 12000] loss: 0.748\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 47 %\n"
     ]
    }
   ],
   "source": [
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the parameters of the network - Number of filters (2 points)\n",
    "\n",
    "Use the previous ConvNet and change the number of filters used in the convolutional layer.\n",
    "\n",
    "(16 or 32 are good options to try!)\n",
    "\n",
    "Describe what happens when you change the number of filters. Do more or fewer do better?\n",
    "\n",
    "**Answer**:  \n",
    "**TODO** \n",
    "More filters do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet2(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Nfilters = 16\n",
    "Ksize = 5\n",
    "padding = 2\n",
    "class ConvNet2(nn.Module):\n",
    "    \"\"\"\n",
    "        The CNN convolutional network with architecture defined above\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # START TODO #############\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters, kernel_size=Ksize,padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features= 16*16*Nfilters, out_features=10)\n",
    "        # END TODO #############\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: The input tensor with shape (batch_size, feature_dim)\n",
    "            The input to the network will be a minibatch of data\n",
    "                \n",
    "        Returns:\n",
    "            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
    "        \"\"\"\n",
    "        # START TODO #############\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0],16*16*Nfilters)\n",
    "        x = self.fc1(x)\n",
    "        # END TODO #############\n",
    "        return x\n",
    "\n",
    "net = ConvNet2()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.941\n",
      "[1,  2000] loss: 0.808\n",
      "[1,  3000] loss: 0.761\n",
      "[1,  4000] loss: 0.726\n",
      "[1,  5000] loss: 0.710\n",
      "[1,  6000] loss: 0.698\n",
      "[1,  7000] loss: 0.687\n",
      "[1,  8000] loss: 0.674\n",
      "[1,  9000] loss: 0.664\n",
      "[1, 10000] loss: 0.657\n",
      "[1, 11000] loss: 0.632\n",
      "[1, 12000] loss: 0.649\n",
      "[2,  1000] loss: 0.580\n",
      "[2,  2000] loss: 0.581\n",
      "[2,  3000] loss: 0.579\n",
      "[2,  4000] loss: 0.596\n",
      "[2,  5000] loss: 0.590\n",
      "[2,  6000] loss: 0.582\n",
      "[2,  7000] loss: 0.572\n",
      "[2,  8000] loss: 0.601\n",
      "[2,  9000] loss: 0.603\n",
      "[2, 10000] loss: 0.590\n",
      "[2, 11000] loss: 0.594\n",
      "[2, 12000] loss: 0.590\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 59 %\n"
     ]
    }
   ],
   "source": [
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the parameters of the network - Filter size (2 points)\n",
    "\n",
    "the filter size in the last network we use 5x5 filters, now use 3x3 filters and describe what happens, is the network more efficient?\n",
    "\n",
    "**Answer**:  \n",
    "**TODO**  \n",
    "Network is more efficient with smaller filters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet3(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Nfilters = 16\n",
    "Ksize = 3\n",
    "padding = 1\n",
    "class ConvNet3(nn.Module):\n",
    "    \"\"\"\n",
    "        The CNN convolutional network with architecture defined above\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # START TODO #############\n",
    "        # Define the layers need to build the network\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters, kernel_size=Ksize,padding=padding)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features= 16*16*Nfilters, out_features=10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            x: The input tensor with shape (batch_size, feature_dim)\n",
    "            The input to the network will be a minibatch of data\n",
    "                \n",
    "        Returns:\n",
    "            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
    "        \"\"\"\n",
    "        # START TODO #############\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Remember to flatten the feature map using x.view\n",
    "        # must have dimentions: N, \n",
    "        x = x.view(x.shape[0],16*16*Nfilters)\n",
    "        x = self.fc1(x)\n",
    "        # END TODO #############\n",
    "        return x\n",
    "\n",
    "net = ConvNet3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 0.954\n",
      "[1,  2000] loss: 0.837\n",
      "[1,  3000] loss: 0.776\n",
      "[1,  4000] loss: 0.743\n",
      "[1,  5000] loss: 0.727\n",
      "[1,  6000] loss: 0.694\n",
      "[1,  7000] loss: 0.694\n",
      "[1,  8000] loss: 0.683\n",
      "[1,  9000] loss: 0.655\n",
      "[1, 10000] loss: 0.686\n",
      "[1, 11000] loss: 0.660\n",
      "[1, 12000] loss: 0.645\n",
      "[2,  1000] loss: 0.590\n",
      "[2,  2000] loss: 0.603\n",
      "[2,  3000] loss: 0.616\n",
      "[2,  4000] loss: 0.612\n",
      "[2,  5000] loss: 0.600\n",
      "[2,  6000] loss: 0.600\n",
      "[2,  7000] loss: 0.595\n",
      "[2,  8000] loss: 0.592\n",
      "[2,  9000] loss: 0.613\n",
      "[2, 10000] loss: 0.591\n",
      "[2, 11000] loss: 0.589\n",
      "[2, 12000] loss: 0.582\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 58 %\n"
     ]
    }
   ],
   "source": [
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization (2 points)\n",
    "\n",
    "Include batch normalization and describe what happens, is the network more efficient?\n",
    "\n",
    "**Answer**:  \n",
    "**TODO**  \n",
    "\n",
    "Batch Normalization: Accelerates Deep Network Training by Reducing Internal Covariate Shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet4(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Nfilters = 16\n",
    "Ksize = 3\n",
    "padding = 1\n",
    "class ConvNet4(nn.Module):\n",
    "    \"\"\"\n",
    "        The CNN convolutional network with architecture defined above\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # START TODO #############\n",
    "        # Define the layers need to build the network\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters, kernel_size=Ksize,padding=padding)\n",
    "        self.conv1_bn = nn.BatchNorm2d(Nfilters)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features= 16*16*Nfilters, out_features=10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            x: The input tensor with shape (batch_size, feature_dim)\n",
    "            The input to the network will be a minibatch of data\n",
    "                \n",
    "        Returns:\n",
    "            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n",
    "        \"\"\"\n",
    "        # START TODO #############\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # Remember to flatten the feature map using x.view\n",
    "        # must have dimentions: N, \n",
    "        x = x.view(x.shape[0],16*16*Nfilters)\n",
    "        x = self.fc1(x)\n",
    "        # END TODO #############\n",
    "        return x\n",
    "\n",
    "net = ConvNet4()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.052\n",
      "[1,  2000] loss: 0.812\n",
      "[1,  3000] loss: 0.760\n",
      "[1,  4000] loss: 0.745\n",
      "[1,  5000] loss: 0.721\n",
      "[1,  6000] loss: 0.709\n",
      "[1,  7000] loss: 0.698\n",
      "[1,  8000] loss: 0.691\n",
      "[1,  9000] loss: 0.685\n",
      "[1, 10000] loss: 0.665\n",
      "[1, 11000] loss: 0.657\n",
      "[1, 12000] loss: 0.675\n",
      "[2,  1000] loss: 0.600\n",
      "[2,  2000] loss: 0.617\n",
      "[2,  3000] loss: 0.630\n",
      "[2,  4000] loss: 0.628\n",
      "[2,  5000] loss: 0.613\n",
      "[2,  6000] loss: 0.600\n",
      "[2,  7000] loss: 0.607\n",
      "[2,  8000] loss: 0.607\n",
      "[2,  9000] loss: 0.604\n",
      "[2, 10000] loss: 0.598\n",
      "[2, 11000] loss: 0.595\n",
      "[2, 12000] loss: 0.590\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 58 %\n"
     ]
    }
   ],
   "source": [
    "# Define a Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(2):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design your own ConvNet (4 points)\n",
    "\n",
    "Taking into account the previous results, design your own ConvNet to achieve at least 70% of accuracy in max 10 epochs.\n",
    "\n",
    "You can change the network architecture. The most common ConvNet architecture follows the pattern:\n",
    "\n",
    "INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC\n",
    "where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N >= 0 (and usually N <= 3), M >= 0, K >= 0 (and usually K < 3)\n",
    "\n",
    "But consider that deeper networks will take a lot of time to train.\n",
    "\n",
    "You can also change the loss function and the optimizer.\n",
    "\n",
    "\n",
    "**Describe what you did:**  \n",
    "**TODO**  \n",
    "\n",
    "INPUT -> [[CONV -> RELU] -> POOL] -> [CONV -> RELU] -> [[CONV -> RELU] -> POOL] -> [FC -> RELU]*2 -> FC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet5(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=2048, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "Nfilters = 32\n",
    "Ksize = 3\n",
    "padding = 1\n",
    "class ConvNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, Ksize, padding=padding)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, Ksize, padding=padding)\n",
    "        self.conv3 = nn.Conv2d(32, 32, Ksize, padding=padding)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 32, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #print(x.size())\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #print(x.size())\n",
    "        x = x.view(4, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ConvNet5()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  1000] loss: 1.151\n",
      "[1,  2000] loss: 1.128\n",
      "[1,  3000] loss: 1.019\n",
      "[1,  4000] loss: 0.948\n",
      "[1,  5000] loss: 0.870\n",
      "[1,  6000] loss: 0.836\n",
      "[1,  7000] loss: 0.781\n",
      "[1,  8000] loss: 0.748\n",
      "[1,  9000] loss: 0.738\n",
      "[1, 10000] loss: 0.712\n",
      "[1, 11000] loss: 0.699\n",
      "[1, 12000] loss: 0.697\n",
      "[2,  1000] loss: 0.649\n",
      "[2,  2000] loss: 0.638\n",
      "[2,  3000] loss: 0.617\n",
      "[2,  4000] loss: 0.618\n",
      "[2,  5000] loss: 0.603\n",
      "[2,  6000] loss: 0.594\n",
      "[2,  7000] loss: 0.578\n",
      "[2,  8000] loss: 0.580\n",
      "[2,  9000] loss: 0.568\n",
      "[2, 10000] loss: 0.553\n",
      "[2, 11000] loss: 0.542\n",
      "[2, 12000] loss: 0.541\n",
      "[3,  1000] loss: 0.493\n",
      "[3,  2000] loss: 0.490\n",
      "[3,  3000] loss: 0.504\n",
      "[3,  4000] loss: 0.494\n",
      "[3,  5000] loss: 0.481\n",
      "[3,  6000] loss: 0.473\n",
      "[3,  7000] loss: 0.477\n",
      "[3,  8000] loss: 0.467\n",
      "[3,  9000] loss: 0.457\n",
      "[3, 10000] loss: 0.449\n",
      "[3, 11000] loss: 0.463\n",
      "[3, 12000] loss: 0.454\n",
      "[4,  1000] loss: 0.396\n",
      "[4,  2000] loss: 0.407\n",
      "[4,  3000] loss: 0.405\n",
      "[4,  4000] loss: 0.401\n",
      "[4,  5000] loss: 0.411\n",
      "[4,  6000] loss: 0.399\n",
      "[4,  7000] loss: 0.391\n",
      "[4,  8000] loss: 0.388\n",
      "[4,  9000] loss: 0.404\n",
      "[4, 10000] loss: 0.402\n",
      "[4, 11000] loss: 0.405\n",
      "[4, 12000] loss: 0.402\n",
      "[5,  1000] loss: 0.323\n",
      "[5,  2000] loss: 0.344\n",
      "[5,  3000] loss: 0.351\n",
      "[5,  4000] loss: 0.332\n",
      "[5,  5000] loss: 0.352\n",
      "[5,  6000] loss: 0.336\n",
      "[5,  7000] loss: 0.365\n",
      "[5,  8000] loss: 0.346\n",
      "[5,  9000] loss: 0.335\n",
      "[5, 10000] loss: 0.354\n",
      "[5, 11000] loss: 0.350\n",
      "[5, 12000] loss: 0.352\n",
      "[6,  1000] loss: 0.282\n",
      "[6,  2000] loss: 0.271\n",
      "[6,  3000] loss: 0.293\n",
      "[6,  4000] loss: 0.296\n",
      "[6,  5000] loss: 0.280\n",
      "[6,  6000] loss: 0.310\n",
      "[6,  7000] loss: 0.312\n",
      "[6,  8000] loss: 0.297\n",
      "[6,  9000] loss: 0.311\n",
      "[6, 10000] loss: 0.302\n",
      "[6, 11000] loss: 0.321\n",
      "[6, 12000] loss: 0.312\n",
      "[7,  1000] loss: 0.226\n",
      "[7,  2000] loss: 0.235\n",
      "[7,  3000] loss: 0.244\n",
      "[7,  4000] loss: 0.254\n",
      "[7,  5000] loss: 0.250\n",
      "[7,  6000] loss: 0.257\n",
      "[7,  7000] loss: 0.260\n",
      "[7,  8000] loss: 0.268\n",
      "[7,  9000] loss: 0.277\n",
      "[7, 10000] loss: 0.260\n",
      "[7, 11000] loss: 0.273\n",
      "[7, 12000] loss: 0.278\n",
      "[8,  1000] loss: 0.195\n",
      "[8,  2000] loss: 0.195\n",
      "[8,  3000] loss: 0.198\n",
      "[8,  4000] loss: 0.208\n",
      "[8,  5000] loss: 0.220\n",
      "[8,  6000] loss: 0.229\n",
      "[8,  7000] loss: 0.240\n",
      "[8,  8000] loss: 0.239\n",
      "[8,  9000] loss: 0.227\n",
      "[8, 10000] loss: 0.231\n",
      "[8, 11000] loss: 0.231\n",
      "[8, 12000] loss: 0.245\n",
      "[9,  1000] loss: 0.160\n",
      "[9,  2000] loss: 0.163\n",
      "[9,  3000] loss: 0.178\n",
      "[9,  4000] loss: 0.183\n",
      "[9,  5000] loss: 0.181\n",
      "[9,  6000] loss: 0.205\n",
      "[9,  7000] loss: 0.215\n",
      "[9,  8000] loss: 0.207\n",
      "[9,  9000] loss: 0.199\n",
      "[9, 10000] loss: 0.221\n",
      "[9, 11000] loss: 0.221\n",
      "[9, 12000] loss: 0.219\n",
      "[10,  1000] loss: 0.136\n",
      "[10,  2000] loss: 0.150\n",
      "[10,  3000] loss: 0.144\n",
      "[10,  4000] loss: 0.164\n",
      "[10,  5000] loss: 0.164\n",
      "[10,  6000] loss: 0.178\n",
      "[10,  7000] loss: 0.189\n",
      "[10,  8000] loss: 0.180\n",
      "[10,  9000] loss: 0.189\n",
      "[10, 10000] loss: 0.186\n",
      "[10, 11000] loss: 0.192\n",
      "[10, 12000] loss: 0.196\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 70 %\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(10):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
