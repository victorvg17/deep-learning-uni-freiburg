{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"ex8_ConNet_architecture.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MJPYjhMAdLtQ","colab_type":"text"},"source":["# Exercise 8\n","This exercise focuses on CNN architectures. We'll continue using Pytorch.\n","\n","We will:\n","\n","- Implement basic architectures.\n","- Experiment with architectures, hyperparameters and batch normalization.\n","- Design and implement a Convolutional network that improves classification accuracy."]},{"cell_type":"code","metadata":{"id":"WNzch7UldLtY","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKjvnT6ydLto","colab_type":"text"},"source":["### Loading and normalizing the CIFAR10 dataset\n","\n","The CIFAR-10 dataset consists of 60000 colour images in 10 classes, 50,000 training images and 10,000 test images of size 32 x 32.\n","\n","https://www.cs.toronto.edu/~kriz/cifar.html\n","\n","\n","The pytorch provides a package called torchvision, that automatically download the CIFAR-10 dataset, preprocess it, and iterate through it in minibatches.\n"]},{"cell_type":"code","metadata":{"id":"spzij3egdLts","colab_type":"code","outputId":"e09713fa-67ec-4dba-e7a9-da8b6fd02268","executionInfo":{"status":"ok","timestamp":1577617856501,"user_tz":-60,"elapsed":2357,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["batch_size = 4\n","\n","transform = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ODAjokZSdLt1","colab_type":"code","outputId":"7c27cd4c-54db-44e3-e320-1b1451316289","executionInfo":{"status":"ok","timestamp":1577617856992,"user_tz":-60,"elapsed":2055,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}},"colab":{"base_uri":"https://localhost:8080/","height":155}},"source":["# get some random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","img = torchvision.utils.make_grid(images[:4,:,:,:])\n","\n","# show images\n","img = img / 2 + 0.5     # unnormalize\n","npimg = img.numpy()\n","plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","plt.show()\n","\n","# print labels\n","print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n","\n"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXAAAAB5CAYAAAAgYXpDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":[" ship   cat truck  bird\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BheuG0OTdLt-","colab_type":"text"},"source":["### Define a Convolutional Neural Network\n","\n","A simple Convolutional Network is a sequence of layers. The most used layers to build ConvNets are Convolutional Layer, Pooling Layer, and Fully-Connected Layer.\n","\n","Your job is to stack the three main types of layers to form a full ConvNet architecture following the pattern in CIFAR-10:\n","\n"," [INPUT - CONV - RELU - POOL - FC]\n"," \n","* INPUT [32x32x3]: the RGB raw pixel values of the image.\n","* CONV Convolutional layer with 3 filters and filter size 5 (use padding value 2 to preserve the input dimension).\n","* RELU applies an elementwise activation function.\n","* POOL downsampling operation through the spatial dimensions kernel size = 2.\n","* FC fully-connected layer that computes the class scores, with output size [1x1x10], where each of the 10 numbers corresponds to a class score. Each neuron in this layer will be connected to all the numbers in the previous volume.\n","\n","Implement the `forward` function, remember that the `backward` pass is computed automatically in Pytorch\n","\n"]},{"cell_type":"code","metadata":{"id":"zobl4c6fMMfX","colab_type":"code","outputId":"4fb5bfbd-fb96-45e0-fdca-2b057c4beee4","executionInfo":{"status":"ok","timestamp":1577617878323,"user_tz":-60,"elapsed":826,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["Nfilters = 3\n","Ksize = 5\n","padding = 2\n","class ConvNet1(nn.Module):\n","    \"\"\"\n","        The CNN convolutional network with architecture defined above\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        # START TODO #############\n","        # initialize required parameters / layers needed to build the network\n","        # our batch size of input = 32 *32 * 3\n","        #input = 32*32*3, ouput = 32*32*3\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=5,stride=1,padding=2)\n","        #input = 32*32*3, ouput = 16*16*3\n","        self.pool = nn.MaxPool2d(kernel_size=2)\n","        #in_features = 16*16*3, out_features=10\n","        self.fc = nn.Linear(in_features=768, out_features=10)\n","        # END TODO #############\n","        \n","\n","    def forward(self, x):\n","        \"\"\"\n","        \n","        Args:\n","            x: The input tensor with shape [batch_size, feature_dim] (minibatch of data)\n","        Returns:\n","            scores: Pytorch tensor of shape (N, C) giving classification scores for x\n","        \"\"\"\n","        # START TODO #############\n","        # batch_size, feature_dim = x.shape\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        N, ht, wt, ch = x.shape\n","        # Remember to flatten the feature map using:\n","        # x = x.view(batch_size, dim)\n","        x = x.view(N, ht*wt*ch)\n","        x = self.fc(x)\n","        # END TODO #############\n","        return x\n","\n","net = ConvNet1()\n","print(net)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["ConvNet1(\n","  (conv1): Conv2d(3, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc): Linear(in_features=768, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a_hZrHnydLuJ","colab_type":"text"},"source":["Now let's train and see the result of the network. If your implementation is correct, you should get around 47% accuracy."]},{"cell_type":"code","metadata":{"id":"NTzdLESNdLuN","colab_type":"code","outputId":"262626fb-3a49-40e8-8629-176867d2489a","executionInfo":{"status":"ok","timestamp":1577617988025,"user_tz":-60,"elapsed":106463,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}},"colab":{"base_uri":"https://localhost:8080/","height":469}},"source":["# Define a Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train the network\n","for epoch in range(2):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 1000 == 999:    # print every 1000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","print('Finished Training')\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[1,  1000] loss: 0.990\n","[1,  2000] loss: 0.894\n","[1,  3000] loss: 0.863\n","[1,  4000] loss: 0.848\n","[1,  5000] loss: 0.842\n","[1,  6000] loss: 0.812\n","[1,  7000] loss: 0.814\n","[1,  8000] loss: 0.825\n","[1,  9000] loss: 0.806\n","[1, 10000] loss: 0.810\n","[1, 11000] loss: 0.802\n","[1, 12000] loss: 0.795\n","[2,  1000] loss: 0.777\n","[2,  2000] loss: 0.774\n","[2,  3000] loss: 0.772\n","[2,  4000] loss: 0.781\n","[2,  5000] loss: 0.771\n","[2,  6000] loss: 0.768\n","[2,  7000] loss: 0.770\n","[2,  8000] loss: 0.757\n","[2,  9000] loss: 0.770\n","[2, 10000] loss: 0.789\n","[2, 11000] loss: 0.768\n","[2, 12000] loss: 0.753\n","Finished Training\n","Accuracy of the network on the 10000 test images: 48 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RWaSyva1dLuY","colab_type":"text"},"source":["### Changing the parameters of the network - Number of filters\n","\n","Use the previous ConvNet and change the number of filters used in the convolutional layer.\n","\n","(16 or 32 are good options to try!)\n","\n","Describe what happens when you change the number of filters. Do more or fewer do better?\n","\n","**Answer**:  \n","**TODO**  "]},{"cell_type":"code","metadata":{"id":"4ICgIEjDdLua","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"c5f8cbd0-be76-490a-f2aa-26032e25487d","executionInfo":{"status":"ok","timestamp":1577622760926,"user_tz":-60,"elapsed":720,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}}},"source":["Nfilters = 16\n","Ksize = 5\n","padding = 2\n","class ConvNet2(nn.Module):\n","    \"\"\"\n","        The CNN convolutional network with architecture defined above\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        # START TODO #############\n","        # initialize required parameters / layers needed to build the network\n","        # our batch size of input = 32 *32 * 3\n","        # input = 32*32*3, ouput = 32*32*16\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters, kernel_size=Ksize, stride=1, padding=2)\n","        #input = 32*32*16, ouput = 16*16*16\n","        self.pool = nn.MaxPool2d(kernel_size=2)\n","        #in_features = 16*16*16, out_features=10\n","        self.fc = nn.Linear(in_features=4096, out_features=10)\n","        # END TODO #############\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x: The input tensor with shape (batch_size, feature_dim)\n","            The input to the network will be a minibatch of data\n","                \n","        Returns:\n","            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n","        \"\"\"\n","        # START TODO #############\n","        # batch_size, feature_dim = x.shape\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        N, ht, wt, ch = x.shape\n","        # Remember to flatten the feature map using:\n","        # x = x.view(batch_size, dim)\n","        x = x.view(N, ht*wt*ch)\n","        x = self.fc(x)\n","        # END TODO #############\n","        return x\n","\n","net = ConvNet2()\n","print(net)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["ConvNet2(\n","  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc): Linear(in_features=4096, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wjKOaM_adLuk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":469},"outputId":"4ed77bc6-6de4-41d0-ced1-5ed0bef96cf8","executionInfo":{"status":"ok","timestamp":1577622891984,"user_tz":-60,"elapsed":129891,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}}},"source":["# Define a Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train the network\n","for epoch in range(2):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 1000 == 999:    # print every 1000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","print('Finished Training')\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["[1,  1000] loss: 0.940\n","[1,  2000] loss: 0.814\n","[1,  3000] loss: 0.779\n","[1,  4000] loss: 0.728\n","[1,  5000] loss: 0.730\n","[1,  6000] loss: 0.702\n","[1,  7000] loss: 0.688\n","[1,  8000] loss: 0.685\n","[1,  9000] loss: 0.656\n","[1, 10000] loss: 0.668\n","[1, 11000] loss: 0.645\n","[1, 12000] loss: 0.634\n","[2,  1000] loss: 0.584\n","[2,  2000] loss: 0.596\n","[2,  3000] loss: 0.589\n","[2,  4000] loss: 0.601\n","[2,  5000] loss: 0.599\n","[2,  6000] loss: 0.591\n","[2,  7000] loss: 0.610\n","[2,  8000] loss: 0.598\n","[2,  9000] loss: 0.590\n","[2, 10000] loss: 0.583\n","[2, 11000] loss: 0.598\n","[2, 12000] loss: 0.575\n","Finished Training\n","Accuracy of the network on the 10000 test images: 59 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jm_F4smPdLur","colab_type":"text"},"source":["### Changing the parameters of the network - Filter size\n","\n","The filter size in the last network we use 5x5 filters, now use 3x3 filters and describe what happens, is the network more efficient?\n","\n","**Answer**:  \n","**TODO**  \n","\n"]},{"cell_type":"code","metadata":{"id":"wpUT1BgsdLuu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"b875e920-be46-4a5e-ede1-387fc2ae65b2","executionInfo":{"status":"ok","timestamp":1577628945430,"user_tz":-60,"elapsed":1772,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}}},"source":["# Nfilters = 16\n","Nfilters = 32\n","Ksize = 3\n","padding = 1\n","class ConvNet3(nn.Module):\n","    \"\"\"\n","        The CNN convolutional network with architecture defined above\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        # START TODO #############\n","        # Define the layers need to build the network\n","        # input = 32*32*3, ouput = 32*32*16\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters, kernel_size=Ksize, stride=1, padding=padding)\n","        #input = 32*32*16, ouput = 16*16*16\n","        self.pool = nn.MaxPool2d(kernel_size=2)\n","        #in_features = 16*16*16, out_features=10\n","        self.fc = nn.Linear(in_features=8192, out_features=10)\n","        \n","        # END TODO #############\n","\n","    def forward(self, x):\n","        \"\"\"\n","        \n","        Args:\n","            x: The input tensor with shape (batch_size, feature_dim)\n","            The input to the network will be a minibatch of data\n","                \n","        Returns:\n","            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n","        \"\"\"\n","        # START TODO #############\n","        # batch_size, feature_dim = x.shape\n","        x = F.relu(self.conv1(x))\n","        x = self.pool(x)\n","        N, ht, wt, ch = x.shape\n","        # Remember to flatten the feature map using:\n","        # x = x.view(batch_size, dim)\n","        x = x.view(N, ht*wt*ch)\n","        x = self.fc(x)\n","        \n","        # Remember to flatten the feature map using x.view\n","        # must have dimentions: N, \n","        \n","        # END TODO #############\n","        return x\n","\n","net = ConvNet3()\n","print(net)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["ConvNet3(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc): Linear(in_features=8192, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6SVjkxBidLu0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":469},"outputId":"770eac34-efd4-4604-e832-8bdafecd3e77","executionInfo":{"status":"ok","timestamp":1577629108794,"user_tz":-60,"elapsed":162055,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}}},"source":["# Define a Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train the network\n","for epoch in range(2):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 1000 == 999:    # print every 1000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","print('Finished Training')\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[1,  1000] loss: 0.938\n","[1,  2000] loss: 0.793\n","[1,  3000] loss: 0.735\n","[1,  4000] loss: 0.724\n","[1,  5000] loss: 0.683\n","[1,  6000] loss: 0.675\n","[1,  7000] loss: 0.666\n","[1,  8000] loss: 0.662\n","[1,  9000] loss: 0.626\n","[1, 10000] loss: 0.623\n","[1, 11000] loss: 0.612\n","[1, 12000] loss: 0.591\n","[2,  1000] loss: 0.576\n","[2,  2000] loss: 0.556\n","[2,  3000] loss: 0.580\n","[2,  4000] loss: 0.558\n","[2,  5000] loss: 0.562\n","[2,  6000] loss: 0.576\n","[2,  7000] loss: 0.572\n","[2,  8000] loss: 0.556\n","[2,  9000] loss: 0.557\n","[2, 10000] loss: 0.553\n","[2, 11000] loss: 0.561\n","[2, 12000] loss: 0.555\n","Finished Training\n","Accuracy of the network on the 10000 test images: 60 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"61SM-Z31dLu9","colab_type":"text"},"source":["### Batch normalization\n","\n","Include batch normalization and describe what happens, is the network more efficient?\n","\n","**Answer**:  \n","**TODO**  \n","\n"]},{"cell_type":"code","metadata":{"id":"6X0Ubep0dLu-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"5d4672ce-267e-482d-b1fa-4c0224419a35","executionInfo":{"status":"ok","timestamp":1577632532709,"user_tz":-60,"elapsed":704,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}}},"source":["Nfilters = 32\n","Ksize = 3\n","padding = 1\n","class ConvNet4(nn.Module):\n","    \"\"\"\n","        The CNN convolutional network with architecture defined above\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        # START TODO #############\n","        # Define the layers need to build the network\n","        # input = 32*32*3, ouput = 32*32*32\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters, kernel_size=Ksize, stride=1, padding=padding)\n","        # batch norm layer for conv1\n","        self.conv1_bn = nn.BatchNorm2d(num_features=Nfilters)\n","        #input = 32*32*16, ouput = 16*16*16\n","        self.pool = nn.MaxPool2d(kernel_size=2)\n","        #in_features = 16*16*16, out_features=10\n","        self.fc = nn.Linear(in_features=8192, out_features=10)\n","        # END TODO #############\n","\n","    def forward(self, x):\n","        \"\"\"\n","        \n","        Args:\n","            x: The input tensor with shape (batch_size, feature_dim)\n","            The input to the network will be a minibatch of data\n","                \n","        Returns:\n","            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n","        \"\"\"\n","        # START TODO #############\n","        x = self.conv1_bn(self.conv1(x))\n","        x = F.relu(x)\n","        x = self.pool(x)\n","        N, ht, wt, ch = x.shape\n","        # Remember to flatten the feature map using:\n","        # x = x.view(batch_size, dim)\n","        x = x.view(N, ht*wt*ch)\n","        x = self.fc(x)\n","        \n","        # Remember to flatten the feature map using x.view\n","        # must have dimentions: N, \n","        \n","        # END TODO #############\n","        return x\n","\n","net = ConvNet4()\n","print(net)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["ConvNet4(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc): Linear(in_features=8192, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MB0rbrhHdLvE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":469},"outputId":"4178f7c2-8316-48a4-dd30-85a913f5263b","executionInfo":{"status":"ok","timestamp":1577632751679,"user_tz":-60,"elapsed":215750,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}}},"source":["# Define a Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train the network\n","for epoch in range(2):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 1000 == 999:    # print every 1000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","print('Finished Training')\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[1,  1000] loss: 1.230\n","[1,  2000] loss: 0.817\n","[1,  3000] loss: 0.770\n","[1,  4000] loss: 0.753\n","[1,  5000] loss: 0.720\n","[1,  6000] loss: 0.705\n","[1,  7000] loss: 0.690\n","[1,  8000] loss: 0.662\n","[1,  9000] loss: 0.667\n","[1, 10000] loss: 0.644\n","[1, 11000] loss: 0.641\n","[1, 12000] loss: 0.625\n","[2,  1000] loss: 0.583\n","[2,  2000] loss: 0.590\n","[2,  3000] loss: 0.573\n","[2,  4000] loss: 0.590\n","[2,  5000] loss: 0.567\n","[2,  6000] loss: 0.606\n","[2,  7000] loss: 0.580\n","[2,  8000] loss: 0.562\n","[2,  9000] loss: 0.581\n","[2, 10000] loss: 0.582\n","[2, 11000] loss: 0.572\n","[2, 12000] loss: 0.571\n","Finished Training\n","Accuracy of the network on the 10000 test images: 59 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E1G-ybV-dLvK","colab_type":"text"},"source":["Taking into account the previous results, design your own ConvNet to achieve at least 70% of accuracy in max 10 epochs.\n","\n","You can change the network architecture. The most common ConvNet architecture follows the pattern:\n","\n","INPUT -> [[CONV -> RELU]\\*N -> POOL?]\\*M -> [FC -> RELU]\\*K -> FC\n","where the * indicates repetition, and the POOL? indicates an optional pooling layer. Moreover, N >= 0 (and usually N <= 3), M >= 0, K >= 0 (and usually K < 3)\n","\n","But consider that deeper networks will take a lot of time to train.\n","\n","You can also change the loss function and the optimizer.\n","\n","\n","**Describe what you did:**  \n","**TODO**  \n","\n"]},{"cell_type":"code","metadata":{"id":"6oT71wfvdLvM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"f27a4b47-8073-4e0e-8238-4c5fae85c8e5","executionInfo":{"status":"ok","timestamp":1577634663813,"user_tz":-60,"elapsed":734,"user":{"displayName":"Victor V George Vadakechirayath","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mB1yuriEr4pnfWOKSh2sJA5b8GNUK-hn_R13EaxFQ=s64","userId":"03216522175233251813"}}},"source":["Nfilters1 = 32\n","Nfilters2 = 64\n","Nfilters3 = 256\n","Ksize = 3\n","padding = 1\n","class ConvNet5(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # START TODO #############\n","        # Define the layers need to build the network\n","        #input = 32*32*3, ouput = 32*32*32\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=Nfilters1, kernel_size=Ksize, stride=1, padding=padding)\n","        # batch norm layer for conv1\n","        self.conv1_bn = nn.BatchNorm2d(num_features=Nfilters1)\n","        #input = 32*32*32, ouput = 32*32*64\n","        self.conv2 = nn.Conv2d(in_channels=Nfilters1, out_channels=Nfilters2, kernel_size=Ksize, stride=1, padding=padding)\n","        # batch norm layer for conv1\n","        self.conv2_bn = nn.BatchNorm2d(num_features=Nfilters2)\n","        #input = 32*32*64, ouput = 32*32*256\n","        self.conv3 = nn.Conv2d(in_channels=Nfilters2, out_channels=Nfilters3, kernel_size=Ksize, stride=1, padding=padding)\n","        # batch norm layer for conv1\n","        self.conv3_bn = nn.BatchNorm2d(num_features=Nfilters3)\n","        #input = 32*32*256, ouput = 16*16*256\n","        self.pool = nn.MaxPool2d(kernel_size=2)\n","        #in_features = 16*16*256, out_features=10\n","        self.fc = nn.Linear(in_features=65536, out_features=10)\n","        \n","        # END TODO #############\n","\n","    def forward(self, x):\n","        \"\"\"\n","        \n","        Args:\n","            x: The input tensor with shape (batch_size, feature_dim)\n","            The input to the network will be a minibatch of data\n","                \n","        Returns:\n","            scores: PyTorch Tensor of shape (N, C) giving classification scores for x\n","        \"\"\"\n","        # START TODO #############\n","        x = self.conv1_bn(self.conv1(x))\n","        x = F.relu(x)\n","        x = self.conv2_bn(self.conv2(x))\n","        x = F.relu(x)\n","        x = self.conv3_bn(self.conv3(x))\n","        x = F.relu(x)\n","        x = self.pool(x)\n","        N, ht, wt, ch = x.shape\n","        # Remember to flatten the feature map using:\n","        x = x.view(N, ht*wt*ch)\n","        # x = self.fc(x)\n","        # x = F.relu(x)\n","        # x = self.fc(x)\n","        # x = F.relu(x)\n","        x = self.fc(x)\n","        # END TODO #############\n","        return x\n","\n","\n","net = ConvNet5()\n","print(net)\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["ConvNet5(\n","  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv1_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv3_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc): Linear(in_features=65536, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YR12mhmLdLvT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"edb0748b-fed6-4559-fb25-e588793a918a"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train the network\n","for epoch in range(10):\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        inputs, labels = data\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 1000 == 999:    # print every 1000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 2000))\n","            running_loss = 0.0\n","print('Finished Training')\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1,  1000] loss: 2.687\n","[1,  2000] loss: 0.923\n","[1,  3000] loss: 0.845\n","[1,  4000] loss: 0.816\n","[1,  5000] loss: 0.786\n","[1,  6000] loss: 0.741\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AivMyJTUU34H","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}